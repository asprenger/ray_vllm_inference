# Serve config file
#
# For documentation see: 
# https://docs.ray.io/en/latest/serve/production-guide/config.html

host: 0.0.0.0
port: 8000

applications:
- name: demo_app
  route_prefix: /
  import_path: ray_vllm_inference.vllm_serve:deployment

  runtime_env:
    env_vars:
      HUGGING_FACE_HUB_TOKEN: hf-1234567890
    pip:
    - ray_vllm_inference @ git+https://github.com//asprenger/ray_vllm_inference
  args:
    model: facebook/opt-125m
    # Use Llama-2:
    # model: meta-llama/Llama-2-7b-chat-hf
    #
    # Use Llama-2 with AWS quantization:
    # model: asprenger/meta-llama-Llama-2-7b-chat-hf-gemm-w4-g128-awq
    # quantization: awq
  deployments:
  - name: VLLMInference
    num_replicas: 1
    # Maximum backlog for a single replica
    max_concurrent_queries: 10
    ray_actor_options:
      num_gpus: 1